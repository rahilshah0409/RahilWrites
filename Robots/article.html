<html class="own-font" lang="en" dir="ltr">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">
    <!-- Link to additional CSS-->
    <link rel="stylesheet" href="../article.css">
    <title>Are robots actually going to take over the world?</title>
</head>

<body class="teal lighten-4 indigo-text text-darken-4">
    <div class="parallax-container">
        <div class="parallax">
            <img src="articlepic2.jpg" alt="" class="responsive-img">
        </div>
    </div>

    <header>
        <h2 class="container articletitle center-align">Are robots actually going to take over the world?</h2>
    </header>

    <main class="container own-flow-text">

    <div class="center-align">
        <img class="headline-img" src="headlinerobotphoto.jpg" alt="">
    </div>

        <p>
            The science fiction film “2001: A Space Odyssey” builds up to a intense fight of man vs machine as Dr Dave
            Bowman and other astronauts try to shut down their ship’s computer system, HAL 9000, when the system tries
            to put members of the team in extreme danger. HAL’s explanation for this course of action is that the
            astronauts will jeopardise the mission if they switch HAL off, going against the system’s ultimate aim. This
            popular sci-fi film is not the only one of its kind. Movies like Ex Machina and The Matrix paint a rather
            dark picture where robots turn evil and end up acting against humans, sometimes in violent ways. But is the
            concept of robots turning evil and taking over the world simply science fiction? Or is there a disturbing
            reality that awaits us?
        </p>

        <p>
            The creation of robots is part of the larger field of artificial intelligence (or AI for short), which is
            not limited to physical machines. This field is a broad area of computer science that makes machines carry
            out human based tasks, greatly benefitting society. It is everywhere, from the voice assistant on the phone
            where you may be reading this article, to the algorithms that drive the social media app that you may have
            heard about this very website. However, reading the book “Superintelligence: Paths, Dangers, Strategies” by
            Nick Bostrom made me think twice about the development of this field in the years to come. Will we be safe
            against the increasing power of artificial intelligence?
        </p>

        <p>
            In this book, Bostrom talks about a hypothetical system which he labels as a “superintelligence”:
        </p>

        <p class="center-align">
            <i>
                “an intellect that greatly exceeds the cognitive performance of humans in virtually all domains of
                interest including creativity, wisdom and social skills”
            </i>
        </p>

        <p>
            Currently, machines and computer systems can perform more efficiently than humans at certain tasks. An
            example of this is a program that acts as an email spam filter- a program can do a much better job at
            sorting through emails in your inbox than a human can. This is an example of narrow or weak AI. However, the
            limitation with this is that it can’t think for itself or learn in the same way that a human can. There are
            some areas of the brain, like the cortex, that can’t be matched by machines. However, some people think that
            in the future, we will reach a point where AI won’t only perform particular tasks better than a human but
            will also adapt faster and plan better than a human. A being much more superior than a human in any and
            every way. This is what is meant by artificial superintelligence and is what would need to be created to
            take over the world.
        </p>

        <p>
            So will we ever reach a point in the future where society will be ruled by a superintelligence? This
            thought-provoking question boils down to two debates. Firstly, will we even reach superintelligence in the
            near future? Secondly, even if we do, will we have control over the system? We will explore these questions
            in
            turn.
        </p>

        <div class="center-align">
            <img class="headline-img" src="articlepic1.jpg" alt="">
        </div>

        <h4 class="center-align"><b>Are we actually going to create a superintelligence any time soon?</b></h4>

        <p>
            As of now, the existence of an artificial superintelligence is one that is hypothetical. Some people believe
            that this will never become a reality and hence, the field of AI will never get to the point where a
            computer system is superior enough to take over society. The more popular, but related, view is that even if
            artificial superintelligence becomes a reality, we are still many years away from achieving it. Therefore,
            we have ample time to figure out how to keep safe against this system, so it won’t be a problem when it is
            created. In 2011, Microsoft co-founder Paul Allen explained in an interview that in order to achieve
            artificial superintelligence, we would need a detailed understanding of how the human brain works. As Allen
            puts it, “we are just scraping the surface of this”. His argument, supported by many other academics, comes
            down to the fact that we have such a limited understanding of human intelligence and how the human brain
            works that we won’t create a superintelligence any time soon.
        </p>

        <p>
            On the flip side, famous figures like Elon Musk and Stephen Hawking fundamentally disagree with this and
            think that it is soon upon us. Philosopher Sam Harris explains that due to our strong dependence of
            technology, we, as a society, will continue to try and make developments in the field of AI in any way
            possible. Key inventions like the car were a thing of fiction before they were invented and following from
            those past trends, coupled with humanity’s desire to develop the technology it relies upon, it is only a
            matter of time before sci-fi stories become factual. Furthermore, Harris explains that we don’t even need to
            strive to create a superintelligence- creating an AI with human intelligence is all we need to do. Say we
            reach a point where we have an AI which is as smart as a human researcher in all aspects. Let’s call this
            System X. Electrical circuits are much faster than biological ones and so a task that would take a
            researcher six months could be completed by System X in a week or even less. It can simply process
            information faster. Not only this, but if System X can be made once, it can be made many times. Hence, from
            that point on, progression in the field of AI will accelerate in an unprecedented way, because we have
            essentially created the equivalent of a human that can do the required work a lot faster.
        </p>

        <div class="center-align">
            <img class="headline-img" src="parallax1.jpg" alt="">
        </div>

        <h4 class="center-align"><b>Will we have control over a superintelligence?</b></h4>

        <p>
            Let’s imagine a scenario where society has managed to build an artificial superintelligence (i.e. the answer
            to the first question is that we do create a superintelligence). Will it do what we tell it to? If it
            doesn’t, would we be able to control it before it gets out of hand? Some people believe that because the
            system will fundamentally be made by human programmers and researchers, we can always switch it off if it
            comes to the point where it seems to become malicious. This point is made by scientist Grady Booch in his
            Ted talk “Don’t fear superintelligent AI”. Furthermore, in the development phase, the superintelligence can
            be made to have a structure that limits its direct influence on human society. An example of this is
            explained in Bostrom’s book and is called an oracle. He explains that the superintelligence can be a machine
            that simply gets asked a question by a human and provides the most optimal answer back to the human via a
            screen. In this scenario, the superintelligence can act as an advisor, telling humans what to do and letting
            them act accordingly. Confining the superintelligence to a box and limiting its capability in this way means
            that we won’t allow the system to gain too much influence, and it is hard to imagine a scenario where a
            system of this form will take over the world.
        </p>

        <p>
            This course of reasoning is disputed by many. When it comes to having control over a super-intelligent
            system, many AI researchers like Stuart Russell argue that it is highly unlikely that we will be able to
            activate “the off switch” even if we wanted or needed to. He argues that it would be silly to ignore the
            fact that this system would be much smarter than a human in all aspects. Hence, the chances are that the
            superintelligence would have already foreseen a scenario where its human programmers have disabled it and
            hence have taken steps to prevent that from happening.
        </p>

        <p>
            Furthermore, just because the superintelligence is programmed with a certain goal to act towards does not
            necessarily mean it will act in the way that we expect. In his Ted talk, Bostrom’s argues that at a time
            where artificial superintelligence exists, intelligence should be defined more generally as “an optimisation
            process in which one uses available means to carry out a task in the most efficient way”. The important word
            to note here is efficient. He continues by saying that actions taken by the system centre around efficiency,
            and hence may not act in the way we want. Say we program a superintelligence such that its only goal is to
            make humans smile. While we may initially expect a system with this goal to make jokes and act in a way that
            causes people to smile, a system striving to find the most efficient approach may simply stick electrodes in
            the facial muscles of humans, causing them to smile indefinitely. This approach is technically more
            effective but would be of the disapproval of any human. The point is that just because we tell a
            superintelligence to do something does not mean that it will act in alignment with what we really want when
            we tell the system to do that thing.
        </p>

        <div class="center-align">
            <img class="headline-img" src="articlepic3.jpg" alt="">
        </div>

        <p>
            I look at this question and am almost overwhelmed with the sheer number of debates that surround the topic.
            Having listened and read the opinions of some academics, I am still convinced that we will reach a point in
            time where superintelligence is a reality. I think that steps to ensure the safety of superintelligence is
            crucial, with the key being that we make sure that even if we are not able to tell the system what to do, it
            will still act in a way that is aligned with our core human values.
        </p>

        <p>
            Most importantly, whether it is happening in the next five years or fifty years does not matter- we should
            take bigger steps in ensuring that the artificial intelligence we create is one that does not make us
            inferior. I hope that if you have taken anything from what you have read so far, it is that the safety of AI
            is just as important as its development. What many people fear is that humanity will continue to develop AI
            at an alarming rate without considering the repercussions when a machine becomes more intelligent than we
            are. In this case, we will be underprepared and in serious danger. Sam Harris puts it best- “we [may] be in
            the process of building some sort of God. Now might be a good time to make sure it’s a God we can live
            with”.
        </p>
    </main>

    <div class="parallax-container">
        <div class="parallax">
            <img src="parallax2.jpg" alt="" class="responsive-img">
        </div>
    </div>

    <footer>

    </footer>

    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <!-- Compiled and minified JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>
    <script>
        $(document).ready(function () {
            $('.parallax').parallax();
        });
    </script>

</body>

</html>